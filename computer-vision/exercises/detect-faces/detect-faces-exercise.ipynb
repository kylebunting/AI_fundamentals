{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Analyzing Faces With the Azure Face Service\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the Azure Cognitive Services Face Library\n",
    "\n",
    "To access the Face service from this Python notebook, you need to install the Azure Cognitive Services Face Library. This library is part of the [Azure SDK for Python](https://github.com/Azure/azure-sdk-for-python) GitHub project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pip install azure-cognitiveservices-vision-face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries and reference the services required to execute the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries need to call the Face service and execute the remaining cells in this notebook\n",
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from py_code import faces\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set variables\n",
    "\n",
    "Enter the key and endpoint of your Face Service resource. To use your cognitive services resource, client applications need its endpoint and authentication key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key = '008bc0c83d6840b3badbf662f040925d' #'YOUR_COGNITIVE_SERVICES_KEY'\n",
    "endpoint = 'https://udacity-cog-services.cognitiveservices.azure.com/' #'YOUR_COGNITIVE_SERVICES_ENDPOINT'\n",
    "\n",
    "print('Ready to analyze faces using the Azure Face service at {}.'.format(endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Face client\n",
    "\n",
    "After setting the `key` and `endpoint` values needed to access your Face service, you can instantiate a new client.\n",
    "\n",
    "Execute the cell below to create a `FaceClient` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Face client\n",
    "client = FaceClient(endpoint, CognitiveServicesCredentials(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect faces\n",
    "\n",
    "The `detect_with_stream()` method of the Python SDK allows us to pass in stream data for an image and get the resulting information back from the Face service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Set the path to the image\n",
    "imagePath = os.path.join('test-images', 'women.jpeg')\n",
    "\n",
    "# Detect faces\n",
    "with open(imagePath, mode=\"rb\") as imageData:\n",
    "    faceResults = client.face.detect_with_stream(imageData)\n",
    "\n",
    "# Display the faces\n",
    "faces.show_faces(imagePath, faceResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each detected face is assigned a unique ID, allowing your application to identify each individual face that was detected.\n",
    "\n",
    "Execute the cell below to see the ID for the face of the woman detected in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the image\n",
    "imagePath = os.path.join('test-images', 'woman.jpeg')\n",
    "\n",
    "# Detect faces\n",
    "with open(imagePath, mode=\"rb\") as imageData:\n",
    "    faceResults = client.face.detect_with_stream(imageData)\n",
    "    \n",
    "# Display the faces\n",
    "faces.show_faces(imagePath, faceResults, show_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze facial attributes\n",
    "\n",
    "The Azure Face service is capable of much more than simply detect faces. Using the `return_face_attributes` argument, we can also analyze facial features and expressions. These attributes allow the Face service to predict approximate age and evaluate any emotional expressions present on the face.\n",
    "\n",
    "Execute the cell below to analyze the facial attributes of the woman in our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open an image\n",
    "imagePath = os.path.join('test-images', 'women.jpeg')\n",
    "\n",
    "# Detect faces and specified facial attributes\n",
    "attributes = ['age', 'gender', 'emotion']\n",
    "\n",
    "# Detect faces\n",
    "with open(imagePath, mode=\"rb\") as imageData:\n",
    "    faceResults = client.face.detect_with_stream(imageData, return_face_attributes=attributes)\n",
    "\n",
    "# Display the faces\n",
    "faces.show_faces(imagePath, faceResults, show_attributes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similar faces\n",
    "\n",
    "When faces are detected using the Face service, each one is assigned a unique `Face Id`. Face Ids are used to individually identify face detections. You can use these Ifs to compare a detected face to previously detected faces and find faces with similar features.\n",
    "\n",
    "Run the cell below to compare the man in one image with the man in another, and find a matching face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "faces = reload(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths to the two images to compare\n",
    "imageOnePath = os.path.join('test-images', 'man-similar-01.jpeg')\n",
    "imageTwoPath = os.path.join('test-images', 'man-similar-02.jpeg')\n",
    "\n",
    "# Detect faces in the first image\n",
    "with open(imageOnePath, mode=\"rb\") as imageOneData:\n",
    "    faceOneResults = client.face.detect_with_stream(imageOneData)\n",
    "\n",
    "# Retrieve the first face identified in the image\n",
    "faceOne = faceOneResults[0]\n",
    "\n",
    "# Detect faces in the second image\n",
    "with open(imageTwoPath, mode=\"rb\") as imageTwoData:\n",
    "    faceTwoResults = client.face.detect_with_stream(imageTwoData)\n",
    "\n",
    "# Retrieve the face Ids found in the second image.\n",
    "imageTwoFaceIds = list(map(lambda face: face.face_id, faceTwoResults))\n",
    "\n",
    "# Find faces in image two that are similar to the face in image one\n",
    "similarFaces = client.face.find_similar(face_id=faceOne.face_id, face_ids=imageTwoFaceIds)\n",
    "\n",
    "# Show the face in image 1, and similar faces in image 2\n",
    "faces.show_similar_faces(imageOnePath, faceOne, imageTwoPath, faceTwoResults, similarFaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognize faces\n",
    "\n",
    "We've demonstrated that the Azure Face service can detect faces and analyze facial attributes, and that is is capable of identifying similar faces.\n",
    "\n",
    "Now, let's take a look at what is involved in implementing a facial recognition solution using the Python SDK.\n",
    "\n",
    "in which you train Face to recognize a specific person's face. This can be useful in a variety of scenarios, such as automatically tagging photographs of friends in a social media application, or using facial recognition as part of a biometric identity verification system.\n",
    "\n",
    "To see how this works, let's use facial recognition to identify a friend named Travis.\n",
    "\n",
    "The first step in this process is to create a person group to store our friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide a group Id\n",
    "groupId = 'friends_group'\n",
    "try:\n",
    "    # Delete group if it already exists\n",
    "    client.person_group.delete(groupId)\n",
    "except Exception as ex:\n",
    "    print(ex.message)\n",
    "finally:\n",
    "    client.person_group.create(groupId, 'friends')\n",
    "    print ('Group created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the person group exists, we can add a person for each employee we want to include in the group, and then register multiple photographs of each person so that Face can learn the distinct facial characetristics of each person. Ideally, the images should show the same person in different poses and with different facial expressions.\n",
    "\n",
    "We'll add a single employee called Wendell, and register three photographs of the employee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Add a person (Travis) to the group\n",
    "travis = client.person_group_person.create(groupId, 'Travis')\n",
    "\n",
    "# Get photo's of Travis\n",
    "folder = os.path.join('test-images', 'travis')\n",
    "travisPics = os.listdir(folder)\n",
    "\n",
    "# Register the photos\n",
    "i = 0\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for pic in travisPics:\n",
    "    # Add each photo to person in person group\n",
    "    if pic.endswith('.jpeg'):\n",
    "        imagePath = os.path.join(folder, pic)\n",
    "        \n",
    "        # Add the face detected in the image to the person group\n",
    "        with open(imagePath, mode=\"rb\") as imageData:\n",
    "            client.person_group_person.add_face_from_stream(groupId, travis.person_id, imageData)\n",
    "\n",
    "        # Display each image\n",
    "        img = Image.open(imagePath)\n",
    "        i+=1\n",
    "        a=fig.add_subplot(1,len(travisPics), i)\n",
    "        a.axis('off')\n",
    "        imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the person added and their photographs registered, we can now train the Face service to recognize the person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.person_group.train(groupId)\n",
    "print('Trained!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the model trained to recognize faces in an image and return the person's identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the face IDs in an image\n",
    "imagePath = os.path.join('test-images', 'friend-04.jpeg')\n",
    "\n",
    "# Detect faces\n",
    "with open(imagePath, mode=\"rb\") as imageData:\n",
    "    faceResults = client.face.detect_with_stream(imageData)\n",
    "    \n",
    "imageFaceIds = list(map(lambda face: face.face_id, faceResults))\n",
    "\n",
    "# Get recognized face names\n",
    "faceNames = {}\n",
    "recognizedFaces = client.face.identify(imageFaceIds, groupId)\n",
    "for face in recognizedFaces:\n",
    "    personName = client.person_group_person.get(groupId, face.candidates[0].person_id).name\n",
    "    faceNames[face.face_id] = personName\n",
    "\n",
    "# show recognized faces\n",
    "faces.show_recognized_faces(imagePath, faceResults, faceNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}