{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3 Exercise: Read Text With Computer Vision Service\n",
    "\n",
    "Add a brief description of the work being done in this notebook..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the notebook\n",
    "\n",
    "To access the Custom Vision service from this Python notebook, you need to install the Azure Cognitive Services Custom Vision Library. This library is part of the [Azure SDK for Python](https://github.com/Azure/azure-sdk-for-python) GitHub project.\n",
    "\n",
    "> To learn more, read the [Azure Cognitive Services modules for Python](https://docs.microsoft.com/python/api/overview/azure/cognitive-services?view=azure-python) article in Microsoft Docs.\n",
    "\n",
    "Execute the cell below to install the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install azure-cognitiveservices-vision-computervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to import the libraries and services required to execute the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries need to access the Computer Vision services\n",
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "# Import a few utility libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set variables\n",
    "\n",
    "To access your Computer Vision service, its authentication key and endpoint URL need to be supplied to client applications. Using the values from the `Keys and Endpoints` page for you Computer Vision service, replace the tokenized values in the cell below, as follows:\n",
    "\n",
    "- Retrieve the `Key 1` value for your Computer Vision service in the Azure portal and update the `key` value below.\n",
    "- Retrieve the `Endpoint` value for your Computer Vision in the Azure portal and update the `endpoint` value below.\n",
    "\n",
    "Execute the cell below to set the variables for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'YOUR_COGNITIVE_SERVICES_KEY'\n",
    "endpoint = 'YOUR_COGNITIVE_SERVICES_ENDPOINT'\n",
    "\n",
    "print('Ready to perform OCR using the Computer Vision service at \"{}\" using the key \"{}.\"''\"'.format(endpoint, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Computer Vision client\n",
    "\n",
    "After setting the `key` and `endpoint` needed to access your Computer Vision service, you can instantiate a client.\n",
    "\n",
    "Execute the cell below to create a `ComputerVisionClient` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Computer Vision client\n",
    "client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read text from an image with the OCR API\n",
    "\n",
    "Use the optical character recognition (OCR) API to extract text from images. The image is of a street sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Create a matplotlib figure to display the classification results\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "# Read an image file into a stream\n",
    "path = os.path.join('ocr-images', 'ocr-001.jpg')\n",
    "stream = open(path, \"rb\")\n",
    "\n",
    "# Extract text from the image using the Computer Vision service\n",
    "ocr = client.recognize_printed_text_in_stream(stream)\n",
    "\n",
    "text = ''\n",
    "\n",
    "# Process the OCR text one line at a time\n",
    "for region in ocr.regions:\n",
    "    for line in region.lines:\n",
    "        # Read the OCR'ed text from each line\n",
    "        for word in line.words:\n",
    "            text += word.text + ' '\n",
    "        #print(text.rstrip())\n",
    "\n",
    "# Display the image with its extracted text\n",
    "img = Image.open(path)\n",
    "draw = ImageDraw.Draw(img)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "fig.suptitle(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add bounding boxes\n",
    "\n",
    "Using the OCR API, you can also draw bounding boxes around each unique string of text identified during the OCR process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matplotlib figure to display the classification results\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "# Display the image with its extracted text inside bounding boxes\n",
    "img = Image.open(path)\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "# Process the text one line at a time, drawing the appropriate bounding box\n",
    "for region in ocr.regions:\n",
    "    for line in region.lines:\n",
    "        # Draw the bounding box for each line of text\n",
    "        l,t,w,h = list(map(int, line.bounding_box.split(',')))\n",
    "        draw.rectangle(((l,t), (l+w, t+h)), outline='magenta', width=5)\n",
    "\n",
    "# Show the image with the text locations highlighted by bounding boxes\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "fig.suptitle(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read scanned documents with the Read API\n",
    "\n",
    "The Computer Vision service include the `Read API`, which can be used to read larger amounts of text from scanned documents, for example.\n",
    "\n",
    "To use the `Read API`, you must send an image to the Computer Vision service. This will be read and analyzed asynchronously by the service. This means you must await the results and retrieve them when processing is completed.\n",
    "\n",
    "Execute the cell below to send an image to your Computer Vision service and then wait for the results. When results are available, retrieve and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Read an image file into a stream\n",
    "path = os.path.join('ocr-images', 'covid-diagnosis.jpg')\n",
    "stream = open(path, \"rb\")\n",
    "\n",
    "# Send an async request to read text within the image\n",
    "operation = client.read_in_stream(stream, raw=True)\n",
    "# Extract the operation ID from the response headers\n",
    "locationHeader = operation.headers[\"Operation-Location\"]\n",
    "operationId = locationHeader.split(\"/\")[-1]\n",
    "\n",
    "# Wait for the asynchronous operation to complete\n",
    "while True:\n",
    "    result = client.get_read_result(operationId)\n",
    "    if result.status not in [OperationStatusCodes.running]:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# When the operation has completed successfuly, print each line of text returned to the output\n",
    "if result.status == OperationStatusCodes.succeeded:\n",
    "    for res in result.analyze_result.read_results:\n",
    "        for line in res.lines:\n",
    "            print(line.text)\n",
    "\n",
    "# Display the image analyzed for comparision to the OCR'ed text results\n",
    "print('\\n')\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "img = Image.open(path)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Handwritten Text\n",
    "\n",
    "In addition to printed text, the `Read API` is also capable of reading handwritten text.\n",
    "\n",
    "Execute the cell below to extract text from a handwritten note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Read an image file into a stream\n",
    "path = os.path.join('ocr-images', 'handwriting.jpeg')\n",
    "stream = open(path, \"rb\")\n",
    "\n",
    "# Send an async request to read text within the image\n",
    "operation = client.read_in_stream(stream, raw=True)\n",
    "# Extract the operation ID from the response headers\n",
    "locationHeader = operation.headers[\"Operation-Location\"]\n",
    "operationId = locationHeader.split(\"/\")[-1]\n",
    "\n",
    "# Wait for the asynchronous operation to complete\n",
    "while True:\n",
    "    result = client.get_read_result(operationId)\n",
    "    if result.status not in [OperationStatusCodes.running]:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# When the operation has completed successfuly, print each line of text returned to the output\n",
    "if result.status == OperationStatusCodes.succeeded:\n",
    "    for res in result.analyze_result.read_results:\n",
    "        for line in res.lines:\n",
    "            print(line.text)\n",
    "\n",
    "# Display the image analyzed for comparision to the OCR'ed text results\n",
    "print('\\n')\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "img = Image.open(path)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}