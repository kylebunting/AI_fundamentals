{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Read Text With Computer Vision Service\n",
    "\n",
    "In this notebook, use the `ComputerVisionClient` object found in the Azure Cognitive Services Python SDK to explore the Optical Character Recognition (OCR) capabilities of Azure Computer Vision.\n",
    "\n",
    "> **Important**: Several cells in this notebook contain `TODO` statements, where you need to update variables or enter code to enable the cell to execute without errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the notebook\n",
    "\n",
    "### Install the Azure Cognitive Service Computer Vision Library\n",
    "\n",
    "To access the Computer Vision service from this Python notebook, you need to install the Azure Cognitive Services Computer Vision Library. This library is part of the [Azure SDK for Python](https://github.com/Azure/azure-sdk-for-python) GitHub project.\n",
    "\n",
    "> To learn more, read the [Azure Cognitive Services modules for Python](https://docs.microsoft.com/python/api/overview/azure/cognitive-services?view=azure-python) article in Microsoft Docs.\n",
    "\n",
    "`TODO`: In the cell below, complete the `pip install` command to install the Azure Cognitive Services Computer Vision library, and then execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install # TODO: Complete this line to install the Azure Cognitive Services Computer Vision library azure-cognitiveservices-vision-computervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and services\n",
    "\n",
    "In addition to the Computer Vision library, we will also make use of a few other Python libraries in this notebook. Run the following cell to import the libraries and reference the services required to execute the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries need to access the Computer Vision services\n",
    "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
    "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "\n",
    "# Import a few utility libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set variables\n",
    "\n",
    "To access your Computer Vision service, its authentication key and endpoint URL need to be supplied to client applications.\n",
    "\n",
    "### Retrieve and set the `key` and `endpoint` values from your Custom Vision prediction service.\n",
    "\n",
    "`TODO`: Using the values from the `Keys and Endpoints` page for you Custom Vision prediction service, replace the tokenized values in the cell below, as follows:\n",
    "\n",
    "- Retrieve the `Key 1` value for your prediction cognitive services resource in the Azure portal and update the `key` value below.\n",
    "- Retrieve the `Endpoint` value for your prediction cognitive services resource in the Azure portal and update the `endpoint` value below.\n",
    "\n",
    "> **Important**: You **must** replace the variable values below with the values from your Custom Vision prediction endpoint or the remaining cells in this notebook will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the values assigned to the variables below\n",
    "key = 'YOUR_COGNITIVE_SERVICES_KEY'\n",
    "endpoint = 'YOUR_COGNITIVE_SERVICES_ENDPOINT'\n",
    "\n",
    "print('Ready to perform OCR using the Computer Vision service at \"{}\" using the key \"{}.\"''\"'.format(endpoint, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Computer Vision client\n",
    "\n",
    "After setting the `key` and `endpoint` needed to access your Computer Vision service, you can instantiate a client.\n",
    "\n",
    "`TODO`: Complete the code below to instantiate a new `ComputerVisionClient`, authenticating against your Azure Cogntive Services account, and then execute the cell below to create the `ComputerVisionClient` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Computer Vision client\n",
    "credentials = CognitiveServicesCredentials(key)\n",
    "client = # TODO: Complete this line to create a new Computer Vision Client.  ComputerVisionClient(endpoint, credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read text from an image with the OCR API\n",
    "\n",
    "Use the optical character recognition (OCR) API to extract text from images. The image is of a street sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Create a matplotlib figure to display the classification results\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "# Read an image file into a stream\n",
    "path = os.path.join('images', 'sign.jpg')\n",
    "stream = open(path, \"rb\")\n",
    "\n",
    "# Extract text from the image using the Computer Vision service\n",
    "ocr = client.recognize_printed_text_in_stream(stream)\n",
    "\n",
    "text = ''\n",
    "\n",
    "# Process the OCR text one line at a time\n",
    "for region in ocr.regions:\n",
    "    for line in region.lines:\n",
    "        # Read the OCR'ed text from each line\n",
    "        for word in line.words:\n",
    "            text += word.text + ' '\n",
    "\n",
    "# Display the image with its extracted text\n",
    "img = Image.open(path)\n",
    "draw = ImageDraw.Draw(img)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "fig.suptitle(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add bounding boxes\n",
    "\n",
    "Using the OCR API, you can also draw bounding boxes around each unique string of text identified during the OCR process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matplotlib figure to display the classification results\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "# Display the image with its extracted text inside bounding boxes\n",
    "img = Image.open(path)\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "# Process the text one line at a time, drawing the appropriate bounding box\n",
    "for region in ocr.regions:\n",
    "    for line in region.lines:\n",
    "        # Draw the bounding box for each line of text\n",
    "        l,t,w,h = list(map(int, line.bounding_box.split(',')))\n",
    "        draw.rectangle(((l,t), (l+w, t+h)), outline='magenta', width=5)\n",
    "\n",
    "# Show the image with the text locations highlighted by bounding boxes\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "fig.suptitle(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read scanned documents with the Read API\n",
    "\n",
    "The Computer Vision service include the `Read API`, which can be used to read larger amounts of text from scanned documents, for example.\n",
    "\n",
    "To use the `Read API`, you must send an image to the Computer Vision service. This will be read and analyzed asynchronously by the service. This means you must await the results and retrieve them when processing is completed.\n",
    "\n",
    "Execute the cell below to send an image to your Computer Vision service and then wait for the results. When results are available, retrieve and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Read an image file into a stream\n",
    "path = os.path.join('docs', 'knowledge-mining.pdf')\n",
    "stream = open(path, \"rb\")\n",
    "\n",
    "# Send an async request to read text within the image\n",
    "operation = client.read_in_stream(stream, raw=True)\n",
    "# Extract the operation ID from the response headers\n",
    "locationHeader = operation.headers[\"Operation-Location\"]\n",
    "operationId = locationHeader.split(\"/\")[-1]\n",
    "\n",
    "# Wait for the asynchronous operation to complete\n",
    "while True:\n",
    "    result = client.get_read_result(operationId)\n",
    "    if result.status not in [OperationStatusCodes.running]:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# When the operation has completed successfuly, print each line of text returned to the output\n",
    "if result.status == OperationStatusCodes.succeeded:\n",
    "    for res in result.analyze_result.read_results:\n",
    "        for line in res.lines:\n",
    "            print(line.text)\n",
    "\n",
    "# Display the image analyzed for comparision to the OCR'ed text results\n",
    "print('\\n')\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "img = Image.open(path)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Handwritten Text\n",
    "\n",
    "In addition to printed text, the `Read API` is also capable of reading handwritten text.\n",
    "\n",
    "Execute the cell below to extract text from a handwritten note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Read an image file into a stream\n",
    "path = os.path.join('images', 'not.jpeg')\n",
    "stream = open(path, \"rb\")\n",
    "\n",
    "# Send an async request to read text within the image\n",
    "operation = client.read_in_stream(stream, raw=True)\n",
    "# Extract the operation ID from the response headers\n",
    "locationHeader = operation.headers[\"Operation-Location\"]\n",
    "operationId = locationHeader.split(\"/\")[-1]\n",
    "\n",
    "# Wait for the asynchronous operation to complete\n",
    "while True:\n",
    "    result = client.get_read_result(operationId)\n",
    "    if result.status not in [OperationStatusCodes.running]:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "# When the operation has completed successfuly, print each line of text returned to the output\n",
    "if result.status == OperationStatusCodes.succeeded:\n",
    "    for res in result.analyze_result.read_results:\n",
    "        for line in res.lines:\n",
    "            print(line.text)\n",
    "\n",
    "# Display the image analyzed for comparision to the OCR'ed text results\n",
    "print('\\n')\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "img = Image.open(path)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}